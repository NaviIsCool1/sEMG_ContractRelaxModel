#!/usr/bin/env python3
"""
data_loader.py

Builds tf.data.Datasets for training/validation/testing from your
train.txt, val.txt, test.txt manifests. Each yields batches of
([2000×1] signal, [2000×1] label) windows for sequence‐to‐sequence training.
"""

import numpy as np
import tensorflow as tf

# Manifest files generated by create_splits.py
TRAIN_MANIFEST = "train.txt"
VAL_MANIFEST   = "val.txt"
TEST_MANIFEST  = "test.txt"

BATCH_SIZE = 32
SEQ_LEN    = 2000  # samples per window
AUTOTUNE   = tf.data.AUTOTUNE

def _load_npz(path):
    path = path.numpy().decode()
    data = np.load(path)
    sig = data['signal'].astype(np.float32).reshape(SEQ_LEN, 1)
    lbl = data['label'].astype(np.float32).reshape(SEQ_LEN, 1)
    return sig, lbl

def _tf_parse(path):
    sig, lbl = tf.py_function(_load_npz, [path], [tf.float32, tf.float32])
    sig.set_shape((SEQ_LEN, 1))
    lbl.set_shape((SEQ_LEN, 1))
    return sig, lbl

def get_dataset(manifest_path, batch_size=BATCH_SIZE, shuffle=False):
    ds = tf.data.TextLineDataset(manifest_path)
    if shuffle:
        ds = ds.shuffle(buffer_size=1000, seed=42)
    ds = ds.map(_tf_parse, num_parallel_calls=AUTOTUNE)
    ds = ds.batch(batch_size)
    ds = ds.prefetch(AUTOTUNE)
    return ds

if __name__ == "__main__":
    print("TensorFlow version:", tf.__version__)
    train_ds = get_dataset(TRAIN_MANIFEST, shuffle=True)
    val_ds   = get_dataset(VAL_MANIFEST)
    test_ds  = get_dataset(TEST_MANIFEST)
    print("Training dataset:", train_ds)
    print("Validation dataset:", val_ds)
    print("Test dataset:", test_ds)
